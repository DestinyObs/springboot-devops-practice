# Development Environment Configuration

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "~> 4.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.0"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.0"
    }
  }

  backend "s3" {
    bucket = "all-destinyobs-infra-terraform-states-xyz"
    key    = "dev/user-registration-microservice/terraform.tfstate"
    region         = "us-east-2"
    encrypt        = true
    use_lockfile   = true
  }
}

provider "aws" {
  region = var.aws_region

  default_tags {
    tags = {
      Environment   = var.environment
      Project       = var.project_name
      ManagedBy     = "Terraform"
      Owner         = var.owner
    }
  }
}

# Data sources
data "aws_availability_zones" "available" {
  state = "available"
}

# VPC Module
module "vpc" {
  source = "../../modules/vpc"

  project_name       = var.project_name
  environment        = var.environment
  vpc_cidr           = var.vpc_cidr
  public_subnets     = var.public_subnets
  private_subnets    = var.private_subnets
  availability_zones = slice(data.aws_availability_zones.available.names, 0, 2)
  enable_nat_gateway = var.enable_nat_gateway
}

# Security Module
module "security" {
  source = "../../modules/security"

  project_name         = var.project_name
  environment          = var.environment
  vpc_id               = module.vpc.vpc_id
  ssh_allowed_cidrs    = var.ssh_allowed_cidrs
}

# EC2 Module
module "ec2" {
  source = "../../modules/ec2"

  name_prefix        = "${var.project_name}-${var.environment}"
  instance_count     = var.instance_count
  instance_type      = var.instance_type
  key_name           = var.create_key_pair ? aws_key_pair.main[0].key_name : var.key_pair_name
  security_group_ids = [module.security.app_security_group_id]
  subnet_ids         = module.vpc.public_subnet_ids
  enable_eip         = false  # Set to false for dev
  target_group_arn   = var.enable_alb ? module.alb[0].target_group_arn : null

  common_tags = {
    Environment = var.environment
    Project     = var.project_name
  }
}

# ALB Module
module "alb" {
  count  = var.enable_alb ? 1 : 0
  source = "../../modules/alb"

  name_prefix        = "${var.project_name}-${var.environment}"
  vpc_id             = module.vpc.vpc_id
  subnet_ids         = module.vpc.public_subnet_ids
  security_group_ids = [module.security.alb_security_group_id]
  
  target_port                     = 8080
  listener_port                   = 80
  health_check_path               = "/actuator/health"
  enable_deletion_protection      = false

  common_tags = {
    Environment = var.environment
    Project     = var.project_name
  }
}

# Generate TLS private key for SSH
resource "tls_private_key" "main" {
  count = var.create_key_pair ? 1 : 0

  algorithm = "RSA"
  rsa_bits  = 4096

  provisioner "local-exec" {
    command = "echo '${self.private_key_pem}' > ${var.key_pair_name}.pem && chmod 400 ${var.key_pair_name}.pem"
  }
}

# Key Pair for EC2 instances
resource "aws_key_pair" "main" {
  count = var.create_key_pair ? 1 : 0

  key_name   = var.key_pair_name
  public_key = tls_private_key.main[0].public_key_openssh

  tags = {
    Name        = var.key_pair_name
    Environment = var.environment
    Project     = var.project_name
  }
}

# Generate Ansible inventory dynamically
resource "local_file" "ansible_inventory" {
  content = <<-EOF
[${var.environment}]
${module.ec2.instance_public_ips[0]} ansible_user=ubuntu

[all:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
ansible_ssh_private_key_file=../terraform/environments/${var.environment}/${var.key_pair_name}.pem
EOF
  filename = "../../ansible/inventory/${var.environment}.ini"

  depends_on = [module.ec2]
}

# Generate Ansible variables file
resource "local_file" "ansible_vars" {
  content = <<-EOF
---
# Environment-specific variables generated by Terraform
environment: ${var.environment}
project_name: ${var.project_name}
instance_type: ${var.instance_type}
aws_region: ${var.aws_region}

# EC2 Instance information
instance_count: ${var.instance_count}
instance_ips:
%{ for ip in module.ec2.instance_public_ips ~}
  - ${ip}
%{ endfor ~}

# Load balancer (if enabled)
enable_alb: ${var.enable_alb}
%{ if var.enable_alb ~}
alb_dns_name: ${module.alb[0].alb_dns_name}
%{ endif ~}
EOF
  filename = "../../ansible/group_vars/${var.environment}.yml"

  depends_on = [module.ec2]
}

# Wait for instance to be ready and run Ansible
resource "null_resource" "run_ansible" {
  depends_on = [
    module.ec2,
    local_file.ansible_inventory,
    tls_private_key.main
  ]

  # Wait for SSH to be available
  provisioner "remote-exec" {
    inline = [
      "echo 'SSH is ready!'"
    ]

    connection {
      type        = "ssh"
      host        = module.ec2.instance_public_ips[0]
      user        = "ubuntu"
      private_key = var.create_key_pair ? tls_private_key.main[0].private_key_pem : file(var.key_pair_name)
      timeout     = "10m"
    }
  }

  # Run Ansible playbook
  provisioner "local-exec" {
    command = <<-EOT
      echo "Waiting 60 seconds for instance to fully boot..."
      sleep 60
      echo "Running Ansible configuration..."
      cd ../../ansible
      ansible-playbook -i inventory/${var.environment}.ini playbooks/site.yml
    EOT
  }

  # Trigger re-run when instance changes
  triggers = {
    instance_ids = join(",", module.ec2.instance_ids)
  }
}
